{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import input_data\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "# Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layer + Pooling #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w     = get_weights([3, 3, 1, 32])\n",
    "conv1 = tf.nn.relu(tf.nn.conv2d(X, w, strides=[1, 1, 1, 1], padding='SAME'))             # shape=(?, 28, 28, 32)\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # shape=(?, 14, 14, 32)\n",
    "drop1 = tf.nn.dropout(pool1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layer + Pooling #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w     = get_weights([3, 3, 32, 64])\n",
    "conv2 = tf.nn.relu(tf.nn.conv2d(drop1, w, strides=[1, 1, 1, 1], padding='SAME'))         # shape=(?, 14, 14, 64)\n",
    "pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # shape=(?, 7, 7, 64)\n",
    "drop2 = tf.nn.dropout(pool2, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w        = get_weights([7*7*64, 625])\n",
    "reshaped = tf.reshape(drop2, [-1, 7*7*64]) # shape=(?, 4*4*128)\n",
    "fullycon = tf.nn.relu(tf.matmul(reshaped, w))\n",
    "drop3    = tf.nn.dropout(fullycon, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w          = get_weights([625, 10])\n",
    "output     = tf.matmul(drop3, w)\n",
    "predict_op = tf.argmax(output, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss       = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output, Y))\n",
    "accuracy   = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output, 1), tf.argmax(Y, 1)), tf.float32))\n",
    "train_op   = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Validation split\n",
    "valid_ratio = float(10)/55\n",
    "train_pairs = zip(mnist.train.images, mnist.train.labels)\n",
    "random.shuffle(train_pairs)\n",
    "train_X, train_Y = zip(*train_pairs[int(valid_ratio*len(train_pairs)):])\n",
    "valid_X, valid_Y = zip(*train_pairs[:int(valid_ratio*len(train_pairs))])\n",
    "\n",
    "# Test pairs\n",
    "test_X, test_Y = mnist.test.images, mnist.test.labels\n",
    "\n",
    "# Reshape\n",
    "train_X = np.array(train_X).reshape(-1, 28, 28, 1)  # 28x28x1 images\n",
    "valid_X = np.array(valid_X).reshape(-1, 28, 28, 1)  # 28x28x1 images\n",
    "test_X  = test_X.reshape(-1, 28, 28, 1)   # 28x28x1 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 45000\n",
      "Validation: 10000\n",
      "Test: 10000\n"
     ]
    }
   ],
   "source": [
    "print 'Training:', len(train_X)\n",
    "print 'Validation:', len(valid_X)\n",
    "print 'Test:', len(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100:  0.938519\n",
      "Step 200:  0.356716\n",
      "Step 300:  0.224322\n",
      "Step 400:  0.211896\n",
      "Step 500:  0.165853\n",
      "Step 600:  0.16777\n",
      "Step 700:  0.13493\n",
      "End of epoch 1:\n",
      "Training loss: 0.313605 Validation loss: 0.10713 Test loss: 0.100931\n",
      "Validation accuracy: 96.5244% Test accuracy: 96.8049%\n",
      "\n",
      "Step 100:  0.145502\n",
      "Step 200:  0.125464\n",
      "Step 300:  0.114705\n",
      "Step 400:  0.115172\n",
      "Step 500:  0.0941064\n",
      "Step 600:  0.098099\n",
      "Step 700:  0.0881787\n",
      "End of epoch 2:\n",
      "Training loss: 0.11152 Validation loss: 0.063265 Test loss: 0.059497\n",
      "Validation accuracy: 97.9367% Test accuracy: 98.0669%\n",
      "\n",
      "Step 100:  0.0857653\n",
      "Step 200:  0.0851376\n",
      "Step 300:  0.0774498\n",
      "Step 400:  0.080694\n",
      "Step 500:  0.0711153\n",
      "Step 600:  0.0710368\n",
      "Step 700:  0.0661157\n",
      "End of epoch 3:\n",
      "Training loss: 0.0766571 Validation loss: 0.046273 Test loss: 0.0420134\n",
      "Validation accuracy: 98.5377% Test accuracy: 98.6278%\n",
      "\n",
      "Step 100:  0.066796\n",
      "Step 200:  0.060172\n",
      "Step 300:  0.0531665\n",
      "Step 400:  0.060895\n",
      "Step 500:  0.0477052\n",
      "Step 600:  0.0576722\n",
      "Step 700:  0.0494426\n",
      "End of epoch 4:\n",
      "Training loss: 0.0563817 Validation loss: 0.0367172 Test loss: 0.0347654\n",
      "Validation accuracy: 98.778% Test accuracy: 98.8682%\n",
      "\n",
      "Step 100:  0.044579\n",
      "Step 200:  0.0503809\n",
      "Step 300:  0.04729\n",
      "Step 400:  0.0545382\n",
      "Step 500:  0.0411813\n",
      "Step 600:  0.0474497\n",
      "Step 700:  0.0405104\n",
      "End of epoch 5:\n",
      "Training loss: 0.0466781 Validation loss: 0.0389515 Test loss: 0.0396057\n",
      "Validation accuracy: 98.778% Test accuracy: 98.7079%\n",
      "\n",
      "Step 100:  0.0438074\n",
      "Step 200:  0.0411708\n",
      "Step 300:  0.0371966\n",
      "Step 400:  0.0429377\n",
      "Step 500:  0.0300882\n",
      "Step 600:  0.0375619\n",
      "Step 700:  0.0371525\n",
      "End of epoch 6:\n",
      "Training loss: 0.0385236 Validation loss: 0.034658 Test loss: 0.0346954\n",
      "Validation accuracy: 98.9684% Test accuracy: 98.8181%\n",
      "\n",
      "Step 100:  0.0352884\n",
      "Step 200:  0.0378658\n",
      "Step 300:  0.0314553\n",
      "Step 400:  0.0347768\n",
      "Step 500:  0.0256126\n",
      "Step 600:  0.0306528\n",
      "Step 700:  0.0290595\n",
      "End of epoch 7:\n",
      "Training loss: 0.0321509 Validation loss: 0.0319271 Test loss: 0.0310166\n",
      "Validation accuracy: 98.9283% Test accuracy: 98.9183%\n",
      "\n",
      "Step 100:  0.0317822\n",
      "Step 200:  0.0296532\n",
      "Step 300:  0.0260041\n",
      "Step 400:  0.0321224\n",
      "Step 500:  0.0293683\n",
      "Step 600:  0.0284917\n",
      "Step 700:  0.0226783\n",
      "End of epoch 8:\n",
      "Training loss: 0.028489 Validation loss: 0.034528 Test loss: 0.0326003\n",
      "Validation accuracy: 99.0385% Test accuracy: 98.9383%\n",
      "\n",
      "Step 100:  0.0187933\n",
      "Step 200:  0.0310278\n",
      "Step 300:  0.0227645\n",
      "Step 400:  0.0280281\n",
      "Step 500:  0.0223632\n",
      "Step 600:  0.0262881\n",
      "Step 700:  0.0223793\n",
      "End of epoch 9:\n",
      "Training loss: 0.0244816 Validation loss: 0.0369462 Test loss: 0.0352264\n",
      "Validation accuracy: 98.8982% Test accuracy: 98.8582%\n",
      "\n",
      "Step 100:  0.0261179\n",
      "Step 200:  0.0218399\n",
      "Step 300:  0.0209948\n",
      "Step 400:  0.0239288\n",
      "Step 500:  0.0191636\n",
      "Step 600:  0.0255558\n",
      "Step 700:  0.0176597\n",
      "End of epoch 10:\n",
      "Training loss: 0.0221886 Validation loss: 0.039823 Test loss: 0.0403966\n",
      "Validation accuracy: 98.9283% Test accuracy: 98.7981%\n",
      "\n",
      "Step 100:  0.0193381\n",
      "Step 200:  0.0231149\n",
      "Step 300:  0.0176771\n",
      "Step 400:  0.0240053\n",
      "Step 500:  0.0183205\n",
      "Step 600:  0.0220628\n",
      "Step 700:  0.0181867\n",
      "End of epoch 11:\n",
      "Training loss: 0.0203293 Validation loss: 0.0371093 Test loss: 0.0282948\n",
      "Validation accuracy: 99.0885% Test accuracy: 99.0885%\n",
      "\n",
      "Step 100:  0.0175492\n",
      "Step 200:  0.0183004\n",
      "Step 300:  0.018413\n",
      "Step 400:  0.0241736\n",
      "Step 500:  0.0125623\n",
      "Step 600:  0.0186442\n",
      "Step 700:  0.0166952\n",
      "End of epoch 12:\n",
      "Training loss: 0.0180105 Validation loss: 0.0346491 Test loss: 0.031175\n",
      "Validation accuracy: 99.0885% Test accuracy: 99.0385%\n",
      "\n",
      "Step 100:  0.0162061\n",
      "Step 200:  0.015282\n",
      "Step 300:  0.0176849\n",
      "Step 400:  0.0172818\n",
      "Step 500:  0.014009\n",
      "Step 600:  0.0199086\n",
      "Step 700:  0.0140002\n",
      "End of epoch 13:\n",
      "Training loss: 0.0162815 Validation loss: 0.038449 Test loss: 0.0314513\n",
      "Validation accuracy: 99.0485% Test accuracy: 99.0685%\n",
      "\n",
      "Step 100:  0.0162775\n",
      "Step 200:  0.0200099\n",
      "Step 300:  0.0171236\n",
      "Step 400:  0.0151985\n",
      "Step 500:  0.0119717\n",
      "Step 600:  0.0152325\n",
      "Step 700:  0.0139018\n",
      "End of epoch 14:\n",
      "Training loss: 0.0156308 Validation loss: 0.0403065 Test loss: 0.0293119\n",
      "Validation accuracy: 99.1186% Test accuracy: 99.1286%\n",
      "\n",
      "Step 100:  0.0142186\n",
      "Step 200:  0.0130197\n",
      "Step 300:  0.0150334\n",
      "Step 400:  0.0147521\n",
      "Step 500:  0.0109078\n",
      "Step 600:  0.0164383\n",
      "Step 700:  0.0124301\n",
      "End of epoch 15:\n",
      "Training loss: 0.0137776 Validation loss: 0.0385236 Test loss: 0.0320889\n",
      "Validation accuracy: 99.0685% Test accuracy: 99.1386%\n",
      "\n",
      "Step 100:  0.013125\n",
      "Step 200:  0.0199235\n",
      "Step 300:  0.010422\n",
      "Step 400:  0.0141792\n",
      "Step 500:  0.0105473\n",
      "Step 600:  0.0169524\n",
      "Step 700:  0.0116352\n",
      "End of epoch 16:\n",
      "Training loss: 0.0138256 Validation loss: 0.0383824 Test loss: 0.0376692\n",
      "Validation accuracy: 99.0885% Test accuracy: 98.8281%\n",
      "\n",
      "Step 100:  0.0138869\n",
      "Step 200:  0.013625\n",
      "Step 300:  0.0151841\n",
      "Step 400:  0.0167168\n",
      "Step 500:  0.009491\n",
      "Step 600:  0.011215\n",
      "Step 700:  0.00973149\n",
      "End of epoch 17:\n",
      "Training loss: 0.0127912 Validation loss: 0.0407234 Test loss: 0.0361934\n",
      "Validation accuracy: 99.1687% Test accuracy: 99.0284%\n",
      "\n",
      "Step 100:  0.0163014\n",
      "Step 200:  0.0129142\n",
      "Step 300:  0.0133519\n",
      "Step 400:  0.0136282\n",
      "Step 500:  0.00975521\n",
      "Step 600:  0.0101333\n",
      "Step 700:  0.00773685\n",
      "End of epoch 18:\n",
      "Training loss: 0.0119241 Validation loss: 0.0382902 Test loss: 0.0324685\n",
      "Validation accuracy: 99.2288% Test accuracy: 99.1587%\n",
      "\n",
      "Step 100:  0.0138997\n",
      "Step 200:  0.0161293\n",
      "Step 300:  0.0127846\n",
      "Step 400:  0.0126423\n",
      "Step 500:  0.0101122\n",
      "Step 600:  0.0089768\n",
      "Step 700:  0.0123274\n",
      "End of epoch 19:\n",
      "Training loss: 0.0123748 Validation loss: 0.040301 Test loss: 0.0332653\n",
      "Validation accuracy: 99.0485% Test accuracy: 99.1587%\n",
      "\n",
      "Step 100:  0.00978627\n",
      "Step 200:  0.00638502\n",
      "Step 300:  0.00956881\n",
      "Step 400:  0.00945693\n",
      "Step 500:  0.00925247\n",
      "Step 600:  0.0125291\n",
      "Step 700:  0.0115059\n",
      "End of epoch 20:\n",
      "Training loss: 0.00974733 Validation loss: 0.0415289 Test loss: 0.0302038\n",
      "Validation accuracy: 99.1286% Test accuracy: 99.1787%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(20):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    valid_acc  = []\n",
    "    test_loss  = []\n",
    "    test_acc   = []\n",
    "\n",
    "    for j in range(len(train_X)/batch_size):\n",
    "        start = j * batch_size\n",
    "        end   = (j+1) * batch_size\n",
    "        loss_, _ = sess.run([loss, train_op], feed_dict={X: train_X[start:end], Y: train_Y[start:end], keep_prob: 0.8})\n",
    "\n",
    "        train_loss.append(loss_)\n",
    "        \n",
    "        if j > 0 and j % 100 == 0:\n",
    "            print ('Step %d: ' % j), np.mean(train_loss[(j-100):j])\n",
    "\n",
    "    for j in range(len(valid_X)/batch_size):\n",
    "        start = j * batch_size\n",
    "        end   = (j+1) * batch_size\n",
    "        loss_, acc_ = sess.run([loss, accuracy], feed_dict={X: valid_X[start:end], Y: valid_Y[start:end], keep_prob: 1.0})\n",
    "\n",
    "        valid_loss.append(loss_)\n",
    "        valid_acc.append(acc_)\n",
    "\n",
    "    for j in range(len(test_X)/batch_size):\n",
    "        start = j * batch_size\n",
    "        end   = (j+1) * batch_size\n",
    "        loss_, acc_ = sess.run([loss, accuracy], feed_dict={X: test_X[start:end], Y: test_Y[start:end], keep_prob: 1.0})\n",
    "\n",
    "        test_loss.append(loss_)\n",
    "        test_acc.append(acc_)\n",
    "\n",
    "    print 'End of epoch %d:' % (i+1)\n",
    "    print 'Training loss:', np.mean(train_loss), \\\n",
    "          'Validation loss:', np.mean(valid_loss), \\\n",
    "          'Test loss:', np.mean(test_loss)\n",
    "    print ('Validation accuracy: %g%%' % (np.mean(valid_acc) * 100)), \\\n",
    "          ('Test accuracy: %g%%' % (np.mean(test_acc) * 100))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe2761c9d90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4AMBEXwgIIIPBPR/+zLZMCO3/8wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe276498d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Input\n",
    "this_input, this_label = random.sample(zip(test_X, test_Y), 1)[0]\n",
    "label = sess.run(predict_op, feed_dict={X: [this_input], Y: [np.zeros_like(this_label)], keep_prob: 1.0})\n",
    "imshow(np.reshape(this_input, (28, 28)), interpolation='none', cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print label[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
