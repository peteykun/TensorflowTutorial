{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import input_data\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Validation split\n",
    "valid_ratio = float(10)/55\n",
    "train_pairs = zip(mnist.train.images, mnist.train.labels)\n",
    "random.shuffle(train_pairs)\n",
    "train_X, train_Y = zip(*train_pairs[int(valid_ratio*len(train_pairs)):])\n",
    "valid_X, valid_Y = zip(*train_pairs[:int(valid_ratio*len(train_pairs))])\n",
    "\n",
    "# Test pairs\n",
    "test_X, test_Y = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 45000\n",
      "Validation: 10000\n",
      "Test: 10000\n"
     ]
    }
   ],
   "source": [
    "print 'Training:', len(train_X)\n",
    "print 'Validation:', len(valid_X)\n",
    "print 'Test:', len(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weights(shape, interval):\n",
    "    val = np.random.uniform(-interval, interval, size=shape)\n",
    "    val = np.cast['float32'](val)\n",
    "    \n",
    "    return tf.Variable(val)\n",
    "\n",
    "def get_bias(shape, offset=0):\n",
    "    val = np.zeros(shape) - offset\n",
    "    val = np.cast['float32'](val)\n",
    "    \n",
    "    return tf.Variable(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binomial_draw(shape=[1], p=0.5, dtype='float32'):\n",
    "    return tf.select(tf.less(tf.random_uniform(shape=shape, minval=0, maxval=1, dtype='float32'), tf.fill(shape, p)), tf.ones(shape, dtype=dtype), tf.zeros(shape, dtype=dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(tensor, rate=0.2):\n",
    "    # Salt and pepper noise\n",
    "    a = binomial_draw(shape=tf.shape(tensor), p=1-rate, dtype='float32')\n",
    "    b = binomial_draw(shape=tf.shape(tensor), p=0.5, dtype='float32')\n",
    "    z = tf.zeros(tf.shape(tensor), dtype='float32')\n",
    "    c = tf.select(tf.equal(a, z), b, z)\n",
    "    \n",
    "    return tf.add(tf.mul(tensor, a), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binarize input\n",
    "binarized_input = tf.floor(tf.add(input_, tf.fill(tf.shape(input_), 0.5)))\n",
    "\n",
    "# Add noise to input\n",
    "noised_input = add_noise(binarized_input)\n",
    "\n",
    "# Input layer to hidden layer\n",
    "W1 = get_weights([784, hidden_size], np.sqrt(6. / (784 + hidden_size)))\n",
    "b1 = get_bias([hidden_size])\n",
    "\n",
    "# Hidden layer to output layer\n",
    "W2 = get_weights([hidden_size, 784], np.sqrt(6. / (784 + hidden_size)))\n",
    "b2 = get_bias([784])\n",
    "\n",
    "# Activations\n",
    "hidden = tf.sigmoid(tf.matmul(noised_input, W1) + b1)\n",
    "reconstruction = tf.sigmoid(tf.matmul(hidden, W2) + b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "clipped_output         = tf.clip_by_value(reconstruction, 1e-10, 1.0)\n",
    "clipped_1_minus_output = tf.clip_by_value(1 - reconstruction, 1e-10, 1.0)\n",
    "\n",
    "loss = -tf.reduce_sum(input_ * tf.log(clipped_output) + (1 - input_) * tf.log(clipped_1_minus_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "train_op_ae = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1:\n",
      "Training loss: 0.268862433028 Validation loss: 0.24987622046 Test loss: 0.249193970787\n",
      "\n",
      "End of epoch 2:\n",
      "Training loss: 0.233223504832 Validation loss: 0.21536567946 Test loss: 0.214572083445\n",
      "\n",
      "End of epoch 3:\n",
      "Training loss: 0.203104907696 Validation loss: 0.19293178756 Test loss: 0.19186072896\n",
      "\n",
      "End of epoch 4:\n",
      "Training loss: 0.185536034461 Validation loss: 0.179738322956 Test loss: 0.178362003738\n",
      "\n",
      "End of epoch 5:\n",
      "Training loss: 0.174755580609 Validation loss: 0.17092540737 Test loss: 0.169331485909\n",
      "\n",
      "End of epoch 6:\n",
      "Training loss: 0.1671118459 Validation loss: 0.164517810181 Test loss: 0.162911806226\n",
      "\n",
      "End of epoch 7:\n",
      "Training loss: 0.161556074338 Validation loss: 0.159704786268 Test loss: 0.157932240697\n",
      "\n",
      "End of epoch 8:\n",
      "Training loss: 0.157220781872 Validation loss: 0.155887614427 Test loss: 0.154131377511\n",
      "\n",
      "End of epoch 9:\n",
      "Training loss: 0.153583596697 Validation loss: 0.152262590307 Test loss: 0.150331581656\n",
      "\n",
      "End of epoch 10:\n",
      "Training loss: 0.150077850209 Validation loss: 0.149213079318 Test loss: 0.147423331741\n",
      "\n",
      "End of epoch 11:\n",
      "Training loss: 0.147362680986 Validation loss: 0.146597028091 Test loss: 0.144849149299\n",
      "\n",
      "End of epoch 12:\n",
      "Training loss: 0.145335030077 Validation loss: 0.145003915894 Test loss: 0.143096796884\n",
      "\n",
      "End of epoch 13:\n",
      "Training loss: 0.143724577283 Validation loss: 0.143600895416 Test loss: 0.141670422115\n",
      "\n",
      "End of epoch 14:\n",
      "Training loss: 0.142421161066 Validation loss: 0.142514440353 Test loss: 0.140413097784\n",
      "\n",
      "End of epoch 15:\n",
      "Training loss: 0.141364853638 Validation loss: 0.141399705429 Test loss: 0.139495611503\n",
      "\n",
      "End of epoch 16:\n",
      "Training loss: 0.140460247439 Validation loss: 0.140494259036 Test loss: 0.138695414125\n",
      "\n",
      "End of epoch 17:\n",
      "Training loss: 0.139735659082 Validation loss: 0.139954575252 Test loss: 0.138006508382\n",
      "\n",
      "End of epoch 18:\n",
      "Training loss: 0.138555303632 Validation loss: 0.137957484544 Test loss: 0.136082535636\n",
      "\n",
      "End of epoch 19:\n",
      "Training loss: 0.136284464 Validation loss: 0.135630947454 Test loss: 0.133671342235\n",
      "\n",
      "End of epoch 20:\n",
      "Training loss: 0.133263282514 Validation loss: 0.131172985197 Test loss: 0.129277084274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(20):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    test_loss  = []\n",
    "\n",
    "    for j in range(len(train_X)/batch_size):\n",
    "        start = j * batch_size\n",
    "        end   = (j+1) * batch_size\n",
    "        loss_, _ = sess.run([loss, train_op_ae], feed_dict={input_: train_X[start:end]})\n",
    "\n",
    "        train_loss.append(loss_/(batch_size * 784))\n",
    "\n",
    "    for j in range(len(valid_X)/batch_size):\n",
    "        start = j * batch_size\n",
    "        end   = (j+1) * batch_size\n",
    "        loss_ = sess.run(loss, feed_dict={input_: valid_X[start:end]})\n",
    "\n",
    "        valid_loss.append(loss_/(batch_size * 784))\n",
    "\n",
    "    for j in range(len(test_X)/batch_size):\n",
    "        start = j * batch_size\n",
    "        end   = (j+1) * batch_size\n",
    "        loss_ = sess.run(loss, feed_dict={input_: test_X[start:end]})\n",
    "\n",
    "        test_loss.append(loss_/(batch_size * 784))\n",
    "\n",
    "    print 'End of epoch %d:' % (i+1)\n",
    "    print 'Training loss:', np.mean(train_loss), \\\n",
    "          'Validation loss:', np.mean(valid_loss), \\\n",
    "          'Test loss:', np.mean(test_loss)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_normal_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.reshape(reconstruction, shape=(-1, 28, 28, 1))\n",
    "Y = tf.placeholder(\"float\", [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w     = get_random_normal_weights([3, 3, 1, 32])\n",
    "conv1 = tf.nn.relu(tf.nn.conv2d(X, w, strides=[1, 1, 1, 1], padding='SAME'))             # shape=(?, 28, 28, 32)\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # shape=(?, 14, 14, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w     = get_random_normal_weights([3, 3, 32, 64])\n",
    "conv2 = tf.nn.relu(tf.nn.conv2d(pool1, w, strides=[1, 1, 1, 1], padding='SAME'))         # shape=(?, 14, 14, 64)\n",
    "pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  # shape=(?, 7, 7, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w        = get_random_normal_weights([7*7*64, 625])\n",
    "reshaped = tf.reshape(pool2, [-1, 7*7*64]) # shape=(?, 4*4*128)\n",
    "fullycon = tf.nn.relu(tf.matmul(reshaped, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w          = get_random_normal_weights([625, 10])\n",
    "output     = tf.matmul(fullycon, w)\n",
    "predict_op = tf.argmax(output, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss       = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output, Y))\n",
    "accuracy   = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output, 1), tf.argmax(Y, 1)), tf.float32))\n",
    "train_op   = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100:  2.30175\n",
      "Step 200:  2.03381\n",
      "Step 300:  1.732\n",
      "Step 400:  1.60573\n",
      "Step 500:  1.38764\n",
      "Step 600:  0.953321\n",
      "Step 700:  0.789282\n",
      "End of epoch 1:\n",
      "Training loss: 1.54044 Validation loss: 0.725793 Test loss: 0.697515\n",
      "Validation accuracy: 76.242% Test accuracy: 77.2236%\n",
      "\n",
      "Step 100:  0.683511\n",
      "Step 200:  0.622878\n",
      "Step 300:  0.5709\n",
      "Step 400:  0.54238\n",
      "Step 500:  0.489986\n",
      "Step 600:  0.44784\n",
      "Step 700:  0.413452\n",
      "End of epoch 2:\n",
      "Training loss: 0.538667 Validation loss: 0.455589 Test loss: 0.443421\n",
      "Validation accuracy: 85.4467% Test accuracy: 85.9475%\n",
      "\n",
      "Step 100:  0.415566\n",
      "Step 200:  0.375017\n",
      "Step 300:  0.359463\n",
      "Step 400:  0.363865\n",
      "Step 500:  0.334272\n",
      "Step 600:  0.325343\n",
      "Step 700:  0.310874\n",
      "End of epoch 3:\n",
      "Training loss: 0.35526 Validation loss: 0.305042 Test loss: 0.298381\n",
      "Validation accuracy: 90.5148% Test accuracy: 90.605%\n",
      "\n",
      "Step 100:  0.314843\n",
      "Step 200:  0.276709\n",
      "Step 300:  0.288471\n",
      "Step 400:  0.296554\n",
      "Step 500:  0.279026\n",
      "Step 600:  0.264797\n",
      "Step 700:  0.262252\n",
      "End of epoch 4:\n",
      "Training loss: 0.283505 Validation loss: 0.281746 Test loss: 0.281907\n",
      "Validation accuracy: 90.9255% Test accuracy: 91.2059%\n",
      "\n",
      "Step 100:  0.267779\n",
      "Step 200:  0.248166\n",
      "Step 300:  0.2566\n",
      "Step 400:  0.26018\n",
      "Step 500:  0.237037\n",
      "Step 600:  0.236152\n",
      "Step 700:  0.222047\n",
      "End of epoch 5:\n",
      "Training loss: 0.247173 Validation loss: 0.243525 Test loss: 0.248296\n",
      "Validation accuracy: 92.5982% Test accuracy: 92.1875%\n",
      "\n",
      "Step 100:  0.234156\n",
      "Step 200:  0.229347\n",
      "Step 300:  0.234479\n",
      "Step 400:  0.238703\n",
      "Step 500:  0.223927\n",
      "Step 600:  0.212077\n",
      "Step 700:  0.204632\n",
      "End of epoch 6:\n",
      "Training loss: 0.225696 Validation loss: 0.240642 Test loss: 0.221428\n",
      "Validation accuracy: 92.3478% Test accuracy: 93.0188%\n",
      "\n",
      "Step 100:  0.201894\n",
      "Step 200:  0.198197\n",
      "Step 300:  0.193902\n",
      "Step 400:  0.218542\n",
      "Step 500:  0.196273\n",
      "Step 600:  0.194453\n",
      "Step 700:  0.186015\n",
      "End of epoch 7:\n",
      "Training loss: 0.198808 Validation loss: 0.212179 Test loss: 0.209893\n",
      "Validation accuracy: 93.3794% Test accuracy: 93.5998%\n",
      "\n",
      "Step 100:  0.195338\n",
      "Step 200:  0.183094\n",
      "Step 300:  0.173084\n",
      "Step 400:  0.200965\n",
      "Step 500:  0.185824\n",
      "Step 600:  0.17915\n",
      "Step 700:  0.173409\n",
      "End of epoch 8:\n",
      "Training loss: 0.184907 Validation loss: 0.197388 Test loss: 0.177958\n",
      "Validation accuracy: 93.8101% Test accuracy: 94.2608%\n",
      "\n",
      "Step 100:  0.194606\n",
      "Step 200:  0.16686\n",
      "Step 300:  0.174677\n",
      "Step 400:  0.173181\n",
      "Step 500:  0.164936\n",
      "Step 600:  0.167139\n",
      "Step 700:  0.16867\n",
      "End of epoch 9:\n",
      "Training loss: 0.17298 Validation loss: 0.172573 Test loss: 0.18267\n",
      "Validation accuracy: 94.401% Test accuracy: 94.1907%\n",
      "\n",
      "Step 100:  0.178782\n",
      "Step 200:  0.146339\n",
      "Step 300:  0.166778\n",
      "Step 400:  0.176128\n",
      "Step 500:  0.156386\n",
      "Step 600:  0.164678\n",
      "Step 700:  0.14048\n",
      "End of epoch 10:\n",
      "Training loss: 0.161938 Validation loss: 0.19912 Test loss: 0.193106\n",
      "Validation accuracy: 93.8401% Test accuracy: 93.8802%\n",
      "\n",
      "Step 100:  0.170269\n",
      "Step 200:  0.154615\n",
      "Step 300:  0.157843\n",
      "Step 400:  0.163531\n",
      "Step 500:  0.151002\n",
      "Step 600:  0.151691\n",
      "Step 700:  0.137221\n",
      "End of epoch 11:\n",
      "Training loss: 0.155746 Validation loss: 0.177557 Test loss: 0.169483\n",
      "Validation accuracy: 94.5312% Test accuracy: 94.4712%\n",
      "\n",
      "Step 100:  0.156116\n",
      "Step 200:  0.146262\n",
      "Step 300:  0.150326\n",
      "Step 400:  0.161326\n",
      "Step 500:  0.143939\n",
      "Step 600:  0.147609\n",
      "Step 700:  0.135566\n",
      "End of epoch 12:\n",
      "Training loss: 0.14891 Validation loss: 0.172884 Test loss: 0.176117\n",
      "Validation accuracy: 94.6014% Test accuracy: 94.361%\n",
      "\n",
      "Step 100:  0.135912\n",
      "Step 200:  0.143945\n",
      "Step 300:  0.133869\n",
      "Step 400:  0.151153\n",
      "Step 500:  0.134815\n",
      "Step 600:  0.141782\n",
      "Step 700:  0.120685\n",
      "End of epoch 13:\n",
      "Training loss: 0.137653 Validation loss: 0.163863 Test loss: 0.158495\n",
      "Validation accuracy: 95.022% Test accuracy: 95.1522%\n",
      "\n",
      "Step 100:  0.137872\n",
      "Step 200:  0.127101\n",
      "Step 300:  0.133671\n",
      "Step 400:  0.142487\n",
      "Step 500:  0.128871\n",
      "Step 600:  0.129663\n",
      "Step 700:  0.12804\n",
      "End of epoch 14:\n",
      "Training loss: 0.132856 Validation loss: 0.173141 Test loss: 0.166388\n",
      "Validation accuracy: 94.5513% Test accuracy: 94.6014%\n",
      "\n",
      "Step 100:  0.122854\n",
      "Step 200:  0.132904\n",
      "Step 300:  0.116974\n",
      "Step 400:  0.129468\n",
      "Step 500:  0.131297\n",
      "Step 600:  0.130334\n",
      "Step 700:  0.112672\n",
      "End of epoch 15:\n",
      "Training loss: 0.12561 Validation loss: 0.148593 Test loss: 0.150501\n",
      "Validation accuracy: 95.5028% Test accuracy: 95.4026%\n",
      "\n",
      "Step 100:  0.128318\n",
      "Step 200:  0.119184\n",
      "Step 300:  0.12227\n",
      "Step 400:  0.132169\n",
      "Step 500:  0.114595\n",
      "Step 600:  0.120328\n",
      "Step 700:  0.116678\n",
      "End of epoch 16:\n",
      "Training loss: 0.122169 Validation loss: 0.150545 Test loss: 0.146513\n",
      "Validation accuracy: 95.2825% Test accuracy: 95.2624%\n",
      "\n",
      "Step 100:  0.124792\n",
      "Step 200:  0.113784\n",
      "Step 300:  0.109927\n",
      "Step 400:  0.136379\n",
      "Step 500:  0.108284\n",
      "Step 600:  0.112407\n",
      "Step 700:  0.106971\n",
      "End of epoch 17:\n",
      "Training loss: 0.116453 Validation loss: 0.149846 Test loss: 0.155983\n",
      "Validation accuracy: 95.2023% Test accuracy: 95.022%\n",
      "\n",
      "Step 100:  0.122846\n",
      "Step 200:  0.106863\n",
      "Step 300:  0.111827\n",
      "Step 400:  0.115992\n",
      "Step 500:  0.111642\n",
      "Step 600:  0.11231\n",
      "Step 700:  0.109414\n",
      "End of epoch 18:\n",
      "Training loss: 0.113609 Validation loss: 0.169292 Test loss: 0.162608\n",
      "Validation accuracy: 94.5813% Test accuracy: 94.8718%\n",
      "\n",
      "Step 100:  0.11589\n",
      "Step 200:  0.113433\n",
      "Step 300:  0.107559\n",
      "Step 400:  0.116633\n",
      "Step 500:  0.110679\n",
      "Step 600:  0.109153\n",
      "Step 700:  0.0984231\n",
      "End of epoch 19:\n",
      "Training loss: 0.110484 Validation loss: 0.142858 Test loss: 0.139663\n",
      "Validation accuracy: 95.7532% Test accuracy: 95.7732%\n",
      "\n",
      "Step 100:  0.110578\n",
      "Step 200:  0.105861\n",
      "Step 300:  0.100011\n",
      "Step 400:  0.119942\n",
      "Step 500:  0.106406\n",
      "Step 600:  0.101029\n",
      "Step 700:  0.098252\n",
      "End of epoch 20:\n",
      "Training loss: 0.106374 Validation loss: 0.155261 Test loss: 0.150258\n",
      "Validation accuracy: 95.2524% Test accuracy: 95.4227%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(20):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    valid_acc  = []\n",
    "    test_loss  = []\n",
    "    test_acc   = []\n",
    "\n",
    "    for j in range(len(train_X)/batch_size):\n",
    "        start = j * batch_size\n",
    "        end   = (j+1) * batch_size\n",
    "        loss_, _ = sess.run([loss, train_op], feed_dict={input_: train_X[start:end], Y: train_Y[start:end]})\n",
    "\n",
    "        train_loss.append(loss_)\n",
    "        \n",
    "        if j > 0 and j % 100 == 0:\n",
    "            print ('Step %d: ' % j), np.mean(train_loss[(j-100):j])\n",
    "\n",
    "    for j in range(len(valid_X)/batch_size):\n",
    "        start = j * batch_size\n",
    "        end   = (j+1) * batch_size\n",
    "        loss_, acc_ = sess.run([loss, accuracy], feed_dict={input_: valid_X[start:end], Y: valid_Y[start:end]})\n",
    "\n",
    "        valid_loss.append(loss_)\n",
    "        valid_acc.append(acc_)\n",
    "\n",
    "    for j in range(len(test_X)/batch_size):\n",
    "        start = j * batch_size\n",
    "        end   = (j+1) * batch_size\n",
    "        loss_, acc_ = sess.run([loss, accuracy], feed_dict={input_: test_X[start:end], Y: test_Y[start:end]})\n",
    "\n",
    "        test_loss.append(loss_)\n",
    "        test_acc.append(acc_)\n",
    "\n",
    "    print 'End of epoch %d:' % (i+1)\n",
    "    print 'Training loss:', np.mean(train_loss), \\\n",
    "          'Validation loss:', np.mean(valid_loss), \\\n",
    "          'Test loss:', np.mean(test_loss)\n",
    "    print ('Validation accuracy: %g%%' % (np.mean(valid_acc) * 100)), \\\n",
    "          ('Test accuracy: %g%%' % (np.mean(test_acc) * 100))\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd648bfd390>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd6490dd890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Input\n",
    "this_input, this_label = random.sample(zip(test_X, test_Y), 1)[0]\n",
    "this_noised_input, label = sess.run([noised_input, predict_op], feed_dict={input_: [this_input], Y: [np.zeros_like(this_label)]})\n",
    "imshow(np.reshape(this_noised_input, (28, 28)), interpolation='none', cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "print label[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
